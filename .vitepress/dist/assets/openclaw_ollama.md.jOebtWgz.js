import{_ as i,o as s,c as e,ai as l}from"./chunks/framework.C_v2XVcg.js";const k=JSON.parse('{"title":"接入本地大模型 (Ollama)","description":"","frontmatter":{},"headers":[],"relativePath":"openclaw_ollama.md","filePath":"openclaw_ollama.md"}'),t={name:"openclaw_ollama.md"};function h(n,a,r,o,p,d){return s(),e("div",null,[...a[0]||(a[0]=[l('<h1 id="接入本地大模型-ollama" tabindex="-1">接入本地大模型 (Ollama) <a class="header-anchor" href="#接入本地大模型-ollama" aria-label="Permalink to “接入本地大模型 (Ollama)”">​</a></h1><p>OpenClaw 支持通过 Ollama 接入本地大模型。</p><h2 id="快速设置" tabindex="-1">快速设置 <a class="header-anchor" href="#快速设置" aria-label="Permalink to “快速设置”">​</a></h2><p>使用以下命令快速启动配置：</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> launch</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> openclaw</span></span></code></pre></div><blockquote><p>之前称为 <code>Clawdbot</code>。<code>ollama launch clawdbot</code> 仍然可以作为别名使用。</p></blockquote><p>这将配置 OpenClaw 使用 Ollama 并启动网关。如果网关已经运行，无需进行任何更改，因为网关将自动重新加载更改。</p><h2 id="配置而不启动" tabindex="-1">配置而不启动 <a class="header-anchor" href="#配置而不启动" aria-label="Permalink to “配置而不启动”">​</a></h2><p>如果你只想进行配置而不启动网关，可以使用 <code>--config</code> 参数：</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> launch</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> openclaw</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --config</span></span></code></pre></div><h2 id="英伟达显卡设置" tabindex="-1">英伟达显卡设置 <a class="header-anchor" href="#英伟达显卡设置" aria-label="Permalink to “英伟达显卡设置”">​</a></h2><p>参考链接：<a href="https://b23.tv/4rPacTQ" target="_blank" rel="noreferrer">设置Ollama模型跑在GPU上-哔哩哔哩</a></p><h3 id="系统环境变量" tabindex="-1">系统环境变量 <a class="header-anchor" href="#系统环境变量" aria-label="Permalink to “系统环境变量”">​</a></h3><p>为了确保 Ollama 使用 CUDA，请添加以下系统环境变量(Windows)：</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">  OLLAMA_CUDA:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span></span></code></pre></div>',15)])])}const b=i(t,[["render",h]]);export{k as __pageData,b as default};
